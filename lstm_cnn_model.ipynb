{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9b77914a-1271-42ea-955d-cbd11fd994b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, Layer, \n",
    "                        Bidirectional, MultiHeadAttention, LayerNormalization, Lambda)\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta, time\n",
    "#import talib\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9e53f650-ab66-436e-ba7d-c84ea5cb61f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price       date          price        High         Low          Open  \\\n",
      "3925  2025-06-16  106796.757812  108915.375  104997.625  105555.59375   \n",
      "\n",
      "Price       volume  \n",
      "3925   50366626945  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch Bitcoin data\n",
    "data = yf.download('BTC-USD', start='2014-01-01', end=pd.Timestamp.today())\n",
    "\n",
    "data = data.reset_index()  # Make Date a regular column\n",
    "data.columns = data.columns.droplevel(1)  # Remove the Ticker level from column\n",
    "data = data.rename(columns={'Close':'price', 'Date':'date', 'Volume':'volume'})  # Rename Price\n",
    "btc_original = data.copy()\n",
    "\n",
    "print(data[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9a73a3f8-845b-4b43-b9bf-9a03b8b5d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    avg_gain = gain.rolling(window=window).mean()\n",
    "    avg_loss = loss.rolling(window=window).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_macd(series, slow=26, fast=12, signal=9):\n",
    "    ema_fast = series.ewm(span=fast).mean()\n",
    "    ema_slow = series.ewm(span=slow).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal).mean()\n",
    "    return macd_line - signal_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "59ef71d5-fbe8-4791-8275-a3d217ebb741",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Close'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Close'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[238], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m obv\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 14\u001b[0m data \u001b[38;5;241m=\u001b[39m calculate_obv(data)\n",
      "Cell \u001b[1;32mIn[238], line 5\u001b[0m, in \u001b[0;36mcalculate_obv\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      3\u001b[0m obv \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i] \u001b[38;5;241m>\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m      6\u001b[0m         obv\u001b[38;5;241m.\u001b[39mappend(obv[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i])\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i] \u001b[38;5;241m<\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Close'"
     ]
    }
   ],
   "source": [
    "# Calculate OBV\n",
    "def calculate_obv(df):\n",
    "    obv = [0]\n",
    "    for i in range(1, len(df)):\n",
    "        if df['price'].iloc[i] > df['price'].iloc[i-1]:\n",
    "            obv.append(obv[-1] + df['volume'].iloc[i])\n",
    "        elif df['price'].iloc[i] < df['price'].iloc[i-1]:\n",
    "            obv.append(obv[-1] - df['volume'].iloc[i])\n",
    "        else:\n",
    "            obv.append(obv[-1])\n",
    "    df['obv'] = obv\n",
    "    return df\n",
    "\n",
    "data = calculate_obv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05495963-33fd-4495-979a-d866060787ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add features\n",
    "data['rsi'] = compute_rsi(data['price'], window=14)\n",
    "data['macd'] = compute_macd(data['price'])\n",
    "\n",
    "#On Balance Volume\n",
    "#data['obv'] = talib.OBV(data['price'], data['volume'])\n",
    "\n",
    "#Daily price % increase\n",
    "data['daily_prt'] = data['price'].pct_change()\n",
    "\n",
    "data['ma5'] = data['price'].rolling(5).mean()\n",
    "data['ma5_prt'] = data['ma5'].pct_change()\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82caf718-dbf4-4f01-b452-8a1056a46ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = data['date'].apply(lambda x : pd.to_datetime(x.date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30433620-0c72-43ff-b49c-77325f11334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cab4cc-eda2-4be2-b1b8-d172e3f40405",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc0b8b-c3b9-4c4b-9c87-c6e7f4995136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "#BTC On chain metrics\n",
    "#source: https://www.blockchain.com/en/explorer\n",
    "\n",
    "#difficulty\n",
    "url_diff = 'https://api.blockchain.info/charts/difficulty?timespan=all&rollingAverage=1days&start=2010-01-01&format=json'\n",
    "url_hash = 'https://api.blockchain.info/charts/hash-rate?timespan=all&rollingAverage=1days&start=2010-01-01&format=json'\n",
    "\n",
    "def get_chain_metrics(url, column_name):\n",
    "    response = requests.get(url)\n",
    "    chain_m = []\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        chain_m = pd.DataFrame(data['values']).copy()\n",
    "    \n",
    "        # Convert and format columns\n",
    "        chain_m['date'] = pd.to_datetime(chain_m['x'], unit='s')\n",
    "        chain_m = chain_m[['x', 'y', 'date']] \\\n",
    "             .rename(columns={\n",
    "                 'x': 'time_sec',\n",
    "                 'y': column_name\n",
    "             })\n",
    "    \n",
    "        #fill in between values\n",
    "        # First, set the time_sec as the index to create a complete time series\n",
    "        chain_m.set_index('time_sec', inplace=True)\n",
    "\n",
    "        # calculate days to offset (data is often not of the same day as today)\n",
    "        current_timestamp = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0).timestamp()\n",
    "        day_offset = int(current_timestamp) - chain_m.index.max()\n",
    "\n",
    "        # Create a complete index with all seconds in the range (or whatever your interval should be)\n",
    "        full_index = pd.RangeIndex(start=chain_m.index.min(), stop=chain_m.index.max() + 1, step=86400)  # daily data\n",
    "        \n",
    "        # Reindex to create missing values for all time points\n",
    "        chain_m = chain_m.reindex(full_index)\n",
    "        \n",
    "        # Now interpolate the missing values\n",
    "        chain_m[column_name] = chain_m[column_name].interpolate(method='linear')  # linear interpolation between points\n",
    "        \n",
    "        # For the date column, we can forward fill or create proper dates\n",
    "        # and offset timestep so last entry matches with todays date\n",
    "        chain_m['date'] = pd.to_datetime(chain_m.index + day_offset, unit='s')\n",
    "        chain_m['date'] = chain_m['date'].apply(lambda x : pd.to_datetime(x.date()))\n",
    "        \n",
    "        # Reset index\n",
    "        chain_m.reset_index(inplace=True)\n",
    "        chain_m.rename(columns={'index': 'time_sec'}, inplace=True)\n",
    "        chain_m = chain_m.drop(columns=['time_sec'])\n",
    "        \n",
    "        print(chain_m.head(10))\n",
    "        return chain_m\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "\n",
    "diff_df = get_chain_metrics(url_diff, 'difficulty')\n",
    "hash_df = get_chain_metrics(url_hash, 'hash_rate')\n",
    "\n",
    "print(diff_df.tail())\n",
    "print(hash_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e6121-58f9-4ef3-86f5-b6b05b179e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge on chain metrics\n",
    "data = data.merge(diff_df, on='date', how='outer')  #merge\n",
    "data = data.merge(hash_df, on='date', how='outer')  #merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa87df-ed15-4ddf-a20e-541701ec151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20396958-3b19-45ae-8d87-d416ca82e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3068279-a5c9-4fde-b208-5212750b063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fear and Greed Index data\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "#source: https://alternative.me/crypto/fear-and-greed-index/\n",
    "url = 'https://api.alternative.me/fng/?limit=0'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data_r = response.json()\n",
    "    df_response = pd.DataFrame(data_r['data'])\n",
    "\n",
    "    # Convert and format columns\n",
    "    df_response['date'] = pd.to_datetime(df_response['timestamp'], unit='s')\n",
    "    df_response = df_response[['value', 'value_classification', 'date']] \\\n",
    "         .rename(columns={\n",
    "             'value': 'fng_value',\n",
    "             'value_classification': 'classification'\n",
    "         })\n",
    "    \n",
    "    print(df_response.head(10))\n",
    "    \n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d70df-5b16-4f93-a864-221625a394eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response['fng_value'] = df_response['fng_value'].apply(lambda x: float(x))\n",
    "fng_df = df_response.copy()\n",
    "fng_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317f9ed-ba20-47b4-ae25-ab242fe7e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2018 = data.merge(fng_df, on='date', how='outer')  #merge\n",
    "data_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520a29d-bf39-4c33-a468-2e5fd71ed318",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2018 = data_2018.dropna()\n",
    "data_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091786cd-afdc-4462-9fab-5e21d14024ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change main data\n",
    "data = data_2018.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6afc62f-194c-408b-a170-937928bbc28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6495c6-32a0-461c-8550-5b3bfb6d5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['price', 'rsi', 'macd', 'volume', 'obv', 'fng_value', \n",
    "            'daily_prt', \n",
    "            'difficulty', 'hash_rate',\n",
    "            'ma5',\n",
    "            'ma5_prt']\n",
    "target = 'ma5_prt'\n",
    "\n",
    "# Prepare data for time series prediction\n",
    "# Normalize each feature separately\n",
    "filtered_data = data[features].values\n",
    "\n",
    "scalers = {}\n",
    "for i in range(len(features)):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    filtered_data[:, i:i+1] = scaler.fit_transform(filtered_data[:, i:i+1])\n",
    "    scalers[features[i]] = scaler\n",
    "\n",
    "target_index = features.index(target)\n",
    "\n",
    "#look_back = 70\n",
    "look_back = 50\n",
    "X, y = [], []\n",
    "for i in range(look_back, len(data)):\n",
    "    X.append(filtered_data[i-look_back:i, :])\n",
    "    y.append(filtered_data[i, target_index])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], len(features)))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0b4d2-2f74-449a-98ee-10fb62fbc404",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbb5f2-6eba-476f-9e38-bf14b0867800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create CNN-LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# CNN layers\n",
    "model.add(Conv1D(filters=256, kernel_size=2, activation='relu', input_shape=(look_back, len(features))))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=256, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# LSTM layers\n",
    "model.add(LSTM(units=100, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Attention layer\n",
    "model.add(Dense(units=100, activation='tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.004), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f23ea5-1a92-43be-8c12-809fe31e48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "test_size = 0.05  # Percentage of data to use for testing\n",
    "#test_size = len(X) - look_back -1\n",
    "\n",
    "# Split into train and test sets\n",
    "split = int(len(X) * (1 - test_size))\n",
    "#split = int(len(X) -1)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acefaaf-21c2-42e4-84d5-e1f74efefc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 114\n",
    "batch_size = 32\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False, verbose=1)\n",
    "\n",
    "# Create and train model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    #callbacks=[early_stop],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5272f57-335b-4196-9426-e93b8a159f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "print(f\"y_test shape: {y_test.shape}\")  # Should be (n_samples,)\n",
    "print(f\"Predictions shape: {test_predict.shape}\")  # Should match y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a26fd6-5485-4cc4-9c9f-2bb08e3cdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scaler(scaled_data, features, scalers):\n",
    "    ### Inverse transform\n",
    "    dummy_array = np.zeros((len(scaled_data), len(features)))\n",
    "    dummy_array[:, target_index] = scaled_data.flatten()\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        dummy_array[:, i] = scalers[features[i]].inverse_transform(dummy_array[:, i].reshape(-1, 1)).flatten()\n",
    "        scaled_data = dummy_array[:, target_index]\n",
    "    \n",
    "    return scaled_data\n",
    "\n",
    "train_predict = inverse_scaler(train_predict, features, scalers)\n",
    "test_predict = inverse_scaler(test_predict, features, scalers)\n",
    "y_train = inverse_scaler(y_train, features, scalers)\n",
    "y_test = inverse_scaler(y_test, features, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386293a-f803-4063-a77f-2db6908f5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_predict))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_predict))\n",
    "print(f'Train RMSE: {train_rmse:.2f}')\n",
    "print(f'Test RMSE: {test_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2de71b-cb79-45a9-99da-d35da026cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_predict[0:10])\n",
    "print(test_predict[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8fa61-f9b5-45ef-94ae-88a2fe2944dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot baseline and predictions\n",
    "plt.plot(data['date'], data[target], label='Actual Price')\n",
    "plt.plot(data[look_back:split + look_back]['date'], train_predict, label='Training Prediction')\n",
    "plt.plot(data[split + look_back:]['date'], test_predict, label='Testing Prediction')\n",
    "plt.title('Bitcoin Price Prediction using CNN-LSTM')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cbfa10-ea83-49d4-9803-a4932c0c62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Plot historical data\n",
    "plt.plot(data['date'][-200:], data['volume'][-200:], label='Historical Price', color='blue')  # Last 500 days\n",
    "\n",
    "# Formatting\n",
    "plt.title('Bitcoin Price: History & 60-Day Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c5318-24ac-4482-a787-0001542bbff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3609c2-a52c-48a8-a280-f2cc8c098285",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_future = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e003bd1-5a9d-4188-ab1c-d65aacf8468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527cdb6-7698-488c-82e5-d9e90332d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare last sequence\n",
    "last_sequence = data.iloc[-look_back:].copy()\n",
    "scaled_sequence = last_sequence.copy()\n",
    "\n",
    "# Scale each feature\n",
    "for i, column in enumerate(features):\n",
    "    scaled_sequence[column] = scalers[column].transform(last_sequence[column].values.reshape(-1, 1))\n",
    "\n",
    "predictions = []\n",
    "current_sequence = scaled_sequence[features].values.copy()\n",
    "\n",
    "days_to_predict = 90\n",
    "\n",
    "for i in range(days_to_predict):\n",
    "    # Reshape for prediction\n",
    "    x_input = current_sequence.reshape((1, look_back, len(features)))\n",
    "    \n",
    "    # Make prediction\n",
    "    predicted_price_scaled = model.predict(x_input, verbose=1)\n",
    "    \n",
    "    # Create new row with predicted close and forecasted indicators\n",
    "    new_row = current_sequence[-1].copy()\n",
    "    new_row[0] = predicted_price_scaled[0, 0]  # Update price\n",
    "    \n",
    "    # Inverse transform just the price\n",
    "    predicted_price = scalers[target].inverse_transform(predicted_price_scaled)[0, 0]\n",
    "    last_price = data_future.loc[data_future.index[-1], 'price']\n",
    "    predictions.append((1 + predicted_price) * last_price)\n",
    "\n",
    "    #Add price to data\n",
    "    predicted_row = {'price': (1 + predicted_price) * last_price, \n",
    "                     'rsi': 0, 'macd': 0, 'volume': 0, 'obv': 0, \n",
    "                     'fng_value': 0, 'difficulty': 0, 'hash_rate': 0,\n",
    "                     'ma5': 0, \n",
    "                     'daily_prt': predicted_price}\n",
    "    data_future.loc[len(data_future)] = predicted_row\n",
    "\n",
    "    #Recalculate metrics\n",
    "    data_future['rsi'] = compute_rsi(data_future['price'], window=14)\n",
    "    data_future['macd'] = compute_macd(data_future['price'])\n",
    "\n",
    "    #Volume\n",
    "    data_future.loc[data_future.index[-1], 'volume'] = data_future['volume'].iloc[-1 + i] * random.uniform(-.02, 0.04)\n",
    "    \n",
    "    #On Balance Volume\n",
    "    data_future = calculate_obv(data_future)\n",
    "    #data_future['obv'] = talib.OBV(data_future['price'], data_future['volume'])\n",
    "\n",
    "    #Fear and Greed\n",
    "    data_future.loc[data_future.index[-1], 'fng_value'] = data_future['fng_value'].rolling(window=7).mean().iloc[-1]\n",
    "\n",
    "    #Difficulty\n",
    "    data_future.loc[data_future.index[-1], 'difficulty'] = data_future['difficulty'].iloc[-2] * 1.006\n",
    "\n",
    "    #Hash Rate\n",
    "    data_future.loc[data_future.index[-1], 'hash_rate'] = data_future['hash_rate'].iloc[-2]\n",
    "\n",
    "    #Moving Average 3days\n",
    "    data_future.loc[data_future.index[-1], 'ma5'] = data_future['price'].rolling(window=7).mean().iloc[-1]\n",
    "\n",
    "    #Daily Pcrt Gain\n",
    "    #data_future.loc[data_future.index[-1], 'daily_prt'] = data_future['hash_rate'].iloc[-2]\n",
    "\n",
    "    #Scale last entry and add to new row sequence\n",
    "    new_row[1] = scalers['rsi'].transform([[data_future.loc[data_future.index[-1], 'rsi']]])[0][0] #RSI\n",
    "    new_row[2] = scalers['macd'].transform([[data_future.loc[data_future.index[-1], 'macd']]])[0][0]  #MACD\n",
    "    new_row[3] = scalers['volume'].transform([[data_future.loc[data_future.index[-1], 'volume']]])[0][0]  #Volume\n",
    "    new_row[4] = scalers['obv'].transform([[data_future.loc[data_future.index[-1], 'obv']]])[0][0]  #OBV\n",
    "    new_row[5] = scalers['fng_value'].transform([[data_future.loc[data_future.index[-1], 'fng_value']]])[0][0]  #FNG\n",
    "    new_row[6] = scalers['difficulty'].transform([[data_future.loc[data_future.index[-1], 'difficulty']]])[0][0]  #Difficulty\n",
    "    new_row[7] = scalers['hash_rate'].transform([[data_future.loc[data_future.index[-1], 'hash_rate']]])[0][0]  #Hash Rate\n",
    "    new_row[6] = scalers['ma5'].transform([[data_future.loc[data_future.index[-1], 'ma5']]])[0][0]  #MA3\n",
    "    new_row[7] = scalers['daily_prt'].transform([[data_future.loc[data_future.index[-1], 'daily_prt']]])[0][0]  #Daily %\n",
    "\n",
    "    # Update sequence\n",
    "    current_sequence = np.vstack([current_sequence[1:], new_row])\n",
    "\n",
    "# Create dates for predictions\n",
    "last_date = pd.to_datetime(data.index[-1])  # Convert to pandas Timestamp\n",
    "prediction_dates = [last_date + pd.Timedelta(days=i) for i in range(1, days_to_predict+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b55fd-f86b-4564-b40f-a305dc7958c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(future_df.head())\n",
    "last_date = data['date'][-1:]\n",
    "#print(data['date'][-1:] + pd.Timedelta(days=3))\n",
    "\n",
    "#prediction_dates = [last_date + pd.Timedelta(days=i) for i in range(1, days_to_predict+1)]\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada64953-eb3b-40ef-ae8f-5f2d21c9acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_day = 86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec3537-fa16-4eef-bc21-5a0002d518d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date_timestamp = pd.to_datetime(last_date.values[0]).timestamp()\n",
    "\n",
    "#last_date_timestamp + half a day + a day = next day\n",
    "prediction_dates = [last_date_timestamp + 43200 + 86400 * i for i in range(1, days_to_predict+1)]\n",
    "print(prediction_dates)\n",
    "\n",
    "prediction_dates = [datetime.fromtimestamp(i).date() for i in prediction_dates]\n",
    "print(prediction_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc0861-f4e5-423b-a4f2-531d66ba80a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.fromtimestamp(last_date_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9a072-6231-4327-935f-e3e8c1506389",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dates[0].isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448dd96-2029-4129-bb4c-d6baab4224c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datetime.date(prediction_dates[0].values[0])\n",
    "print(datetime.timestamp(datetime.now()))\n",
    "print(datetime.timestamp(datetime.now()) % timestamp_day)\n",
    "print(datetime.timestamp(datetime.now()) - (datetime.timestamp(datetime.now()) % timestamp_day))\n",
    "print('\\n')\n",
    "print(datetime.timestamp(datetime(2025, 6, 12)))\n",
    "print(datetime.timestamp(datetime(2025, 6, 13)))\n",
    "print(datetime.timestamp(datetime(2025, 6, 14)))\n",
    "print(datetime.timestamp(datetime(2025, 6, 15)))\n",
    "print('\\n')\n",
    "print(datetime.timestamp(datetime(2025, 6, 15)) - datetime.timestamp(datetime(2025, 6, 14)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568ea24-7a07-4aad-a6fd-6e30b7e928e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for future predictions\n",
    "future_df = pd.DataFrame({\n",
    "    'Date': prediction_dates,\n",
    "    'Predicted_Price': predictions\n",
    "})\n",
    "future_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Plot historical data\n",
    "plt.plot(btc_original['date'][-500:], btc_original['price'][-500:], label='Historical Price', color='blue')  # Last 500 days\n",
    "\n",
    "# Plot future predictions\n",
    "plt.plot(prediction_dates, predictions, label='60-Day Prediction', color='red', linestyle='--')\n",
    "\n",
    "# Formatting\n",
    "plt.title('Bitcoin Price: History & 60-Day Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the predictions\n",
    "print(\"\\nPredicted Bitcoin Prices for the Next 60 Days:\")\n",
    "print(future_df.head(10))  # Show first 10 days of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a5668-add4-4a4a-a635-72a8d071f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df = future_df.reset_index()\n",
    "future_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ad992-415e-4f20-bf46-d7079682613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#future_df = future_df.reset_index()\n",
    "#future_df = future_df.rename(columns={'Date':'date', 'Predicted_price':'price'})  # Rename Price\n",
    "future_df = future_df[['Date', 'Predicted_Price']]\n",
    "\n",
    "future_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b2d3c-e108-4b95-8c87-55a6c4c3d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df.to_csv('lstm_cnn_btc_price.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
